{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGEryL8XrnVF6lxRluo1nl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun9438/assignment/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "WtOnl7n5SV2U",
        "outputId": "0dd9e87b-1c68-4711-e588-30153d12e9d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a CNN, filters (kernels) are small learnable matrices that slide over the input image to detect important local features such as edges, textures, and patterns. Each filter extracts a specific type of visual pattern.\\n\\nA feature map (or activation map) is the output produced after applying a filter using convolution, followed by a nonlinear activation function. Feature maps highlight where a certain feature is present in the input image.\\n\\nFilters = pattern detectors\\n\\nFeature maps = visualization of detected patterns\\n\\nTogether, they allow a CNN to learn hierarchical visual representationsâ€”from edges (early layers) to complex shapes and objects (deeper layers).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Question 1: What is the role of filters and feature maps in Convolutional Neural Networks (CNN)?\n",
        "'''In a CNN, filters (kernels) are small learnable matrices that slide over the input image to detect important local features such as edges, textures, and patterns. Each filter extracts a specific type of visual pattern.\n",
        "\n",
        "A feature map (or activation map) is the output produced after applying a filter using convolution, followed by a nonlinear activation function. Feature maps highlight where a certain feature is present in the input image.\n",
        "\n",
        "Filters = pattern detectors\n",
        "\n",
        "Feature maps = visualization of detected patterns\n",
        "\n",
        "Together, they allow a CNN to learn hierarchical visual representationsâ€”from edges (early layers) to complex shapes and objects (deeper layers).'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2: Explain padding and stride in CNNs. How do they affect output feature map dimensions?\n",
        "'''Padding\n",
        "\n",
        "Padding refers to adding extra rows/columns (usually zeros) around the input image.\n",
        "\n",
        "Same Padding â†’ Output size â‰ˆ Input size\n",
        "\n",
        "Valid Padding (No Padding) â†’ Output shrinks\n",
        "\n",
        "Formula:\n",
        "\n",
        "ğ‘‚\n",
        "=\n",
        "(\n",
        "ğ¼\n",
        "âˆ’\n",
        "ğ¾\n",
        "+\n",
        "2\n",
        "ğ‘ƒ\n",
        ")\n",
        "ğ‘†\n",
        "+\n",
        "1\n",
        "O=\n",
        "S\n",
        "(Iâˆ’K+2P)\n",
        "\tâ€‹\n",
        "\n",
        "+1\n",
        "Stride\n",
        "\n",
        "Stride is the number of pixels by which the filter moves during convolution.\n",
        "\n",
        "Stride = 1 â†’ High resolution output\n",
        "\n",
        "Stride > 1 â†’ Output size decreases\n",
        "\n",
        "Effect on Output Size\n",
        "More padding â†’ larger output\n",
        "\n",
        "Higher stride â†’ smaller output\n",
        "\n",
        "Larger kernel â†’ smaller output\n",
        "\n",
        "Padding preserves spatial size; stride controls downsampling.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "jLmfC3woSnjQ",
        "outputId": "a01823ed-c169-433a-95e0-237ccb42006e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Padding\\n\\nPadding refers to adding extra rows/columns (usually zeros) around the input image.\\n\\nSame Padding â†’ Output size â‰ˆ Input size\\n\\nValid Padding (No Padding) â†’ Output shrinks\\n\\nFormula:\\n\\nğ‘‚\\n=\\n(\\nğ¼\\nâˆ’\\nğ¾\\n+\\n2\\nğ‘ƒ\\n)\\nğ‘†\\n+\\n1\\nO=\\nS\\n(Iâˆ’K+2P)\\n\\t\\u200b\\n\\n+1\\nStride\\n\\nStride is the number of pixels by which the filter moves during convolution.\\n\\nStride = 1 â†’ High resolution output\\n\\nStride > 1 â†’ Output size decreases\\n\\nEffect on Output Size\\nMore padding â†’ larger output\\n\\nHigher stride â†’ smaller output\\n\\nLarger kernel â†’ smaller output\\n\\nPadding preserves spatial size; stride controls downsampling.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 3: Define receptive field in CNNs. Why is it important for deep architectures?\n",
        "\n",
        "'''A receptive field is the region of the input image that influences a particular neuron in a feature map.\n",
        "\n",
        "In deeper layers, the receptive field becomes increasingly larger because each convolution builds on previous ones.\n",
        "\n",
        "Importance\n",
        "\n",
        "Allows CNN to understand larger context\n",
        "\n",
        "Helps detect complex patterns & full objects\n",
        "\n",
        "Prevents loss of global information\n",
        "\n",
        "Essential for tasks like detection and classification\n",
        "\n",
        "Deep CNNs have large receptive fields â†’ better object-level understanding.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "H3QFq6jqSyAh",
        "outputId": "898f217c-c63b-4ccf-a566-ef8afedc432d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A receptive field is the region of the input image that influences a particular neuron in a feature map.\\n\\nIn deeper layers, the receptive field becomes increasingly larger because each convolution builds on previous ones.\\n\\nImportance\\n\\nAllows CNN to understand larger context\\n\\nHelps detect complex patterns & full objects\\n\\nPrevents loss of global information\\n\\nEssential for tasks like detection and classification\\n\\nDeep CNNs have large receptive fields â†’ better object-level understanding.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "'''Filter Size\n",
        "\n",
        "A filter of size\n",
        "ğ¾\n",
        "Ã—\n",
        "ğ¾\n",
        "KÃ—K on a layer with\n",
        "ğ¶\n",
        "C input channels has:\n",
        "\n",
        "Parameters\n",
        "=\n",
        "ğ¾\n",
        "2\n",
        "Ã—\n",
        "ğ¶\n",
        "+\n",
        "bias\n",
        "Parameters=K\n",
        "2\n",
        "Ã—C+bias\n",
        "\n",
        "Larger filters â†’ more parameters â†’ higher computational cost.\n",
        "\n",
        "Stride\n",
        "\n",
        "Stride does not change the number of parameters.\n",
        "It only decides how many positions the filter is applied to:\n",
        "\n",
        "Higher stride â†’ fewer output locations â†’ lower compute\n",
        "\n",
        "But parameters remain constant\n",
        "\n",
        "So:\n",
        "\n",
        "Filter size affects number of trainable parameters\n",
        "\n",
        "Stride affects computation, not parameters'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "cAw_HbbXS6YI",
        "outputId": "aa7e9380-b12d-41c4-9f57-4b4aec7fd96b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Filter Size\\n\\nA filter of size \\nğ¾\\nÃ—\\nğ¾\\nKÃ—K on a layer with \\nğ¶\\nC input channels has:\\n\\nParameters\\n=\\nğ¾\\n2\\nÃ—\\nğ¶\\n+\\nbias\\nParameters=K\\n2\\nÃ—C+bias\\n\\nLarger filters â†’ more parameters â†’ higher computational cost.\\n\\nStride\\n\\nStride does not change the number of parameters.\\nIt only decides how many positions the filter is applied to:\\n\\nHigher stride â†’ fewer output locations â†’ lower compute\\n\\nBut parameters remain constant\\n\\nSo:\\n\\nFilter size affects number of trainable parameters\\n\\nStride affects computation, not parameters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5: Compare LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "'''LeNet â†’ Small, simple, grayscale\n",
        "\n",
        "AlexNet â†’ Breakthrough model with big filters\n",
        "\n",
        "VGG â†’ Very deep, small filters, best general-purpose extractor'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GO2Wmt-zTA34",
        "outputId": "0a812aeb-1a9f-449b-bebe-36d99ec2face"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LeNet â†’ Small, simple, grayscale\\n\\nAlexNet â†’ Breakthrough model with big filters\\n\\nVGG â†’ Very deep, small filters, best general-purpose extractor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Keras â€“ Build and train a simple CNN on MNIST\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy =\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJsOuQgCTMVf",
        "outputId": "e27a885a-7fcf-4965-9bbd-4f289f581a6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9063 - loss: 0.3143 - val_accuracy: 0.9815 - val_loss: 0.0667\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 22ms/step - accuracy: 0.9832 - loss: 0.0543 - val_accuracy: 0.9845 - val_loss: 0.0561\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 21ms/step - accuracy: 0.9902 - loss: 0.0323 - val_accuracy: 0.9877 - val_loss: 0.0458\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 20ms/step - accuracy: 0.9936 - loss: 0.0199 - val_accuracy: 0.9887 - val_loss: 0.0449\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 21ms/step - accuracy: 0.9962 - loss: 0.0130 - val_accuracy: 0.9880 - val_loss: 0.0461\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9841 - loss: 0.0547\n",
            "Test Accuracy = 0.986299991607666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Keras â€“ Load & preprocess CIFAR-10 + CNN model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Q-sKrBTQpW",
        "outputId": "73edf248-15a5-4ed5-865d-aeab449be1c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 70ms/step - accuracy: 0.3658 - loss: 1.7331\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 71ms/step - accuracy: 0.5847 - loss: 1.1856\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 69ms/step - accuracy: 0.6571 - loss: 0.9910\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 71ms/step - accuracy: 0.6881 - loss: 0.9012\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.7202 - loss: 0.8099\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.7423 - loss: 0.7494\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.7600 - loss: 0.6942\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 70ms/step - accuracy: 0.7791 - loss: 0.6335\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 71ms/step - accuracy: 0.7956 - loss: 0.5838\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 69ms/step - accuracy: 0.8159 - loss: 0.5306\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8feb238f80>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: PyTorch â€“ Define & train CNN on MNIST\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data loaders\n",
        "transform = transforms.ToTensor()\n",
        "train = datasets.MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test = datasets.MNIST(root='.', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=64)\n",
        "\n",
        "# Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*13*13, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(3):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        output = model(images)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "print(\"Accuracy =\", correct/total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RysC16iaTWdu",
        "outputId": "4d3c1385-2dbb-4c78-dbfe-e3a39eb11aa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 22.4MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 617kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 5.58MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 8.56MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/content/dataset/train\", exist_ok=True)\n",
        "os.makedirs(\"/content/dataset/validation\", exist_ok=True)\n",
        "\n",
        "print(\"Folders ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-WkJSWWYUP9",
        "outputId": "532b5476-2c2a-4d29-835c-cf0c79644903"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "q--sjvAHYZ-v",
        "outputId": "b86fc084-cb5c-49cb-a122-529c615afb03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-579e0d63-7dce-4ac8-8cce-491673c440e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-579e0d63-7dce-4ac8-8cce-491673c440e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pexels-francesco-ungaro-1525041.jpg to pexels-francesco-ungaro-1525041.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Keras ImageDataGenerator â€“ Custom dataset training\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1/255, validation_split=0.2)\n",
        "\n",
        "train = datagen.flow_from_directory(\n",
        "    \"dataset/\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val = datagen.flow_from_directory(\n",
        "    \"dataset/\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32,(3,3),activation='relu',input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2),\n",
        "    Flatten(),\n",
        "    Dense(64,activation='relu'),\n",
        "    Dense(train.num_classes,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train, validation_data=val, epochs=10)\n"
      ],
      "metadata": {
        "id": "t8e9Kmm0Tbqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: End-to-end approach for medical imaging classification (Chest X-ray) + Streamlit deployment\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train = datagen.flow_from_directory(\"chest_xray/train\", target_size=(224,224))\n",
        "test = datagen.flow_from_directory(\"chest_xray/test\", target_size=(224,224))\n",
        "\n",
        "base = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
        "base.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train, validation_data=test, epochs=10)\n",
        "model.save(\"xray_model.h5\")\n"
      ],
      "metadata": {
        "id": "UT3zyj94ThE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S745y0UGTnqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}